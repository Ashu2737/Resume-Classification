# -*- coding: utf-8 -*-
"""deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UJ3TZWXb9rWlXZdWrOwYo882XcAbn3li
"""

import streamlit as st
import os
import pandas as pd
import numpy as np
import pickle
import tempfile
from PIL import Image
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
nltk.download('stopwords')
nltk.download('punkt')
from nltk import word_tokenize
import re
import textract
from sklearn.preprocessing import LabelEncoder as encoder

# Load the trained model from the pickle file
with open(r'C:\Users\Vijaya\rf\classifier_rf.pkl', 'rb') as file:
    model = pickle.load(file)

with open(r'C:\Users\Vijaya\rf\tfidf_vectorizer.pkl', 'rb') as file:
    vectorizer = pickle.load(file)

# Set up the Streamlit app
img = Image.open("Resume.jpeg")
st.image(img)

# Add a file uploader or text input for resume
resume_file = st.file_uploader("Upload your resume", type=["pdf", "docx","doc"])

def cleanResume(resumeText):
    resumeText = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', resumeText)  # remove punctuations
    resumeText = re.sub(r'[^\x00-\x7f]',r' ', resumeText) # remove non-ascii characters
    resumeText = re.sub('\s+', ' ', resumeText)  # remove extra whitespace
    resumeText = re.sub(r'[0-9]+', '', resumeText)  #remove numbers
    return resumeText.lower()

# Function to extract text from the resume
def extract_text(resume_file):
    # Save the uploaded file to a temporary location
    temp_path = os.path.join(".", resume_file.name)
    with open(temp_path, "wb") as f:
        f.write(resume_file.getbuffer())

    # Use textract to extract text from the temporary file
    text = textract.process(temp_path)
    text = text.decode('utf-8') if text else ""

    # Clean the resume text
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    cleaned_text = " ".join(word for word in words if word.lower() not in stop_words and word not in string.punctuation)

    # Delete the temporary file
    os.remove(temp_path)

    return cleaned_text

skill_mapping = {0: 'Peoplesoft resumes', 1:'React Developer', 2: 'SQL Developer Lightning insight', 3: 'workday resumes'}

# Set a title for the deployment page
st.title("Resume Classifier")

# Perform classification on the resume when a button is clicked
if st.button("Classify"):
    if resume_file is not None:
        # Process the uploaded resume file
        preprocessed_resume_text = extract_text(resume_file)

        # Perform preprocessing and vectorization on the resume text
        resume_text_tfidf = vectorizer.transform([preprocessed_resume_text]).toarray()


        # Perform classification on the resume text using the loaded model
        #predicted_skills_encoded = model.predict(resume_text_tfidf)  # Replace with your model prediction code

        predicted_probabilities = model.predict_proba(resume_text_tfidf)



        # Get the predicted skills and their probabilities for each sample (resume)
        predicted_skills = []
        for prob in predicted_probabilities:
            # Find the indices of classes sorted in descending order of probabilities
            sorted_class_indices = np.argsort(prob)[::-1]
            # Get the class names and probabilities in sorted order
            skill_probabilities = [(skill_mapping[idx], prob[idx]) for idx in sorted_class_indices]
            predicted_skills.append(skill_probabilities)

        #  Display the predicted skills and their probabilities
        st.write("Predicted Skills and Probabilities:")
        for idx, skills_probs in enumerate(predicted_skills):
            st.write(f"Resume {idx + 1}:")
            for skill, prob in skills_probs:
                st.write(f"{skill}: {prob:.2f}")
    else:
        st.write("Please upload a resume file.")